{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using seq2seq models to represent duplicate questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook (and the whole project in general) is dedicated to Quora Kaggle competition where I am supposed to build a model to determine whether two given Quora questions are duplicates of each other\n",
    "\n",
    "For me personally this is a great motivation to try out RNNs so my plan for now is to build some basic encoder-decoder model where I take a question and translate it to its duplicate - this way I could represent original questions with ~100-dimensional vectors and carry on with classification task from there\n",
    "\n",
    "However, there is a catch - the training dataset in this competition consists of only ~400K question pairs and it is somewhat conventionally assumed that RNNs are really demanding in terms of training samples and these ~400K may not be enough \n",
    "\n",
    "To circumvent this problem, in my seq2seq model I will focus on word embeddings (i.e. in the decoding stage of my model I will predict not the actual words but their embeddings - as their dimension is way smaller than the number of all words that should drastically decrease the number of model parameters so that I would have a better match with a small training sample) - I believe this approach looks interesting in its own right"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing necessary stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this project all DL related stuff is handled with keras (version 2.0.x whcih is important as I use merge layers in my model) on top of theano (although I do not make use of any theano-specific features yet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, Dense, Dropout, Activation, Embedding, LSTM, RepeatVector, Lambda\n",
    "from keras.layers.merge import Concatenate\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the NLP side I am interested in word embeddings - as of now I am just building model prototype, I decided to generate them on the fly with gensim implementation of word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lyapin roman\\Anaconda2\\envs\\dl_env\\lib\\site-packages\\gensim\\utils.py:855: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from gensim.models import Word2Vec\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing remaining libraries - note cosine_similarity fucntion - later on I manage to get question encodings and compare them between duplicates and non-duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os.path\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', -1)  \n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the corpus and setting model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "WORD_EMBEDDING_DIM = 100\n",
    "QUESTION_ENCODING_DIM = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regarding the corpus for my model: for now I leave the test set aside as my original plan involved translating quesitons to their duplicates (and I need true labels to select those)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of training sample: 404290\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>What is the step by step guide to invest in share market in india?</td>\n",
       "      <td>What is the step by step guide to invest in share market?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>What is the story of Kohinoor (Koh-i-Noor) Diamond?</td>\n",
       "      <td>What would happen if the Indian government stole the Kohinoor (Koh-i-Noor) diamond back?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>How can I increase the speed of my internet connection while using a VPN?</td>\n",
       "      <td>How can Internet speed be increased by hacking through DNS?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>Why am I mentally very lonely? How can I solve it?</td>\n",
       "      <td>Find the remainder when [math]23^{24}[/math] is divided by 24,23?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>Which one dissolve in water quikly sugar, salt, methane and carbon di oxide?</td>\n",
       "      <td>Which fish would survive in salt water?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  qid1  qid2  \\\n",
       "0  0   1     2      \n",
       "1  1   3     4      \n",
       "2  2   5     6      \n",
       "3  3   7     8      \n",
       "4  4   9     10     \n",
       "\n",
       "                                                                      question1  \\\n",
       "0  What is the step by step guide to invest in share market in india?             \n",
       "1  What is the story of Kohinoor (Koh-i-Noor) Diamond?                            \n",
       "2  How can I increase the speed of my internet connection while using a VPN?      \n",
       "3  Why am I mentally very lonely? How can I solve it?                             \n",
       "4  Which one dissolve in water quikly sugar, salt, methane and carbon di oxide?   \n",
       "\n",
       "                                                                                  question2  \\\n",
       "0  What is the step by step guide to invest in share market?                                  \n",
       "1  What would happen if the Indian government stole the Kohinoor (Koh-i-Noor) diamond back?   \n",
       "2  How can Internet speed be increased by hacking through DNS?                                \n",
       "3  Find the remainder when [math]23^{24}[/math] is divided by 24,23?                          \n",
       "4  Which fish would survive in salt water?                                                    \n",
       "\n",
       "   is_duplicate  \n",
       "0  0             \n",
       "1  0             \n",
       "2  0             \n",
       "3  0             \n",
       "4  0             "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv(\"train.csv.zip\", sep=\",\", compression=\"zip\")\n",
    "print(\"The size of training sample: \" + str(len(train_df)))\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Cleaning the corpus and getting word2vec embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To process the corpus I cast everything to lowercase and throw away everything that is not alphanumeric - I also made use of nltk stopwords (however, after inspection it turned out that this stoplist contained words that could define the meaning of the questions - in the future it might make sense to get rid of the stopwords altogether)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stopwords = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "stopwords = stopwords.difference(set([\"can\", \"who\", \"any\", \"which\", \"when\", \"whom\", \n",
    "                                      \"if\", \"what\", \"how\", \"why\", \"where\", \"only\", \"same\", \"more\", \"now\"\n",
    "                                     ]))\n",
    "\n",
    "def process_question(raw_question):\n",
    "    tokens = re.sub(r\"[^a-z0-9]+\", \" \", str(raw_question).lower()).split()\n",
    "    return [x for x in tokens if x not in stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of unique questions in the corpus: 537933\n",
      "[['what', 'step', 'step', 'guide', 'invest', 'share', 'market', 'india'], ['what', 'step', 'step', 'guide', 'invest', 'share', 'market'], ['what', 'story', 'kohinoor', 'koh', 'noor', 'diamond'], ['what', 'would', 'happen', 'if', 'indian', 'government', 'stole', 'kohinoor', 'koh', 'noor', 'diamond', 'back'], ['how', 'can', 'increase', 'speed', 'internet', 'connection', 'using', 'vpn'], ['how', 'can', 'internet', 'speed', 'increased', 'hacking', 'dns'], ['why', 'mentally', 'lonely', 'how', 'can', 'solve'], ['find', 'remainder', 'when', 'math', '23', '24', 'math', 'divided', '24', '23'], ['which', 'one', 'dissolve', 'water', 'quikly', 'sugar', 'salt', 'methane', 'carbon', 'di', 'oxide'], ['which', 'fish', 'would', 'survive', 'salt', 'water']]\n"
     ]
    }
   ],
   "source": [
    "corpus = []\n",
    "seen_ids = set()\n",
    "\n",
    "for index, row in train_df.iterrows():\n",
    "    q1_id = row[\"qid1\"]\n",
    "    if q1_id not in seen_ids:\n",
    "        corpus.append(process_question(row[\"question1\"]))\n",
    "        seen_ids.add(q1_id)\n",
    "    q2_id = row[\"qid2\"]\n",
    "    if q2_id not in seen_ids:\n",
    "        corpus.append(process_question(row[\"question2\"]))\n",
    "        seen_ids.add(q2_id)    \n",
    "        \n",
    "print(\"The number of unique questions in the corpus: \" + str(len(corpus)))\n",
    "print(corpus[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training word2vec model: note that even if I use pretrained embeddings later I will still need to adapt them to the corpus I have in the competition (to see that it is different enough consider \"cat\" token - in this competition \"cat\" refers less to the animal and more to the standardized test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "w2v_embeddings = Word2Vec(corpus, size=WORD_EMBEDDING_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('xat', 0.7465585470199585),\n",
       " ('gmat', 0.7090531587600708),\n",
       " ('aipmt', 0.7055665850639343),\n",
       " ('percentile', 0.7016848921775818),\n",
       " ('clat', 0.6829637289047241),\n",
       " ('bitsat', 0.6823551654815674),\n",
       " ('sat', 0.6673076152801514),\n",
       " ('toefl', 0.666313111782074),\n",
       " ('cmat', 0.665096640586853),\n",
       " ('iift', 0.6648986339569092)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_embeddings.most_similar(positive=[\"cat\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting duplicate question pairs for seq2seq model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to push duplicate questions into keras model I first need to encode them (replace tokens in the original questions with indices) - to be able to track what my questions stand for afterwards I dump words and indices into double linked dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of unique words in the corpus: 85180\n",
      "The size of the longest question: 108\n"
     ]
    }
   ],
   "source": [
    "word2index_map = {}\n",
    "index2word_map = {}\n",
    "max_question_length = 0\n",
    "current_index = 1\n",
    "\n",
    "for index, row in train_df.iterrows():\n",
    "    tokens_a = process_question(row[\"question1\"])\n",
    "    tokens_b = process_question(row[\"question2\"])\n",
    "    max_question_length = max(max_question_length, max(len(tokens_a), len(tokens_b)))\n",
    "    for token in tokens_a + tokens_b:\n",
    "        if token not in word2index_map:\n",
    "            word2index_map[token] = current_index\n",
    "            index2word_map[current_index] = token\n",
    "            current_index += 1\n",
    "            \n",
    "print(\"The number of unique words in the corpus: \" + str(len(word2index_map)))\n",
    "print(\"The size of the longest question: \" + str(max_question_length))           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of duplicate question pairs: 149263\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>Astrology: I am a Capricorn Sun Cap moon and cap rising...what does that say about me?</td>\n",
       "      <td>I'm a triple Capricorn (Sun, Moon and ascendant in Capricorn) What does this say about me?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>15</td>\n",
       "      <td>16</td>\n",
       "      <td>How can I be a good geologist?</td>\n",
       "      <td>What should I do to be a great geologist?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>23</td>\n",
       "      <td>24</td>\n",
       "      <td>How do I read and find my YouTube comments?</td>\n",
       "      <td>How can I see all my Youtube comments?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>25</td>\n",
       "      <td>26</td>\n",
       "      <td>What can make Physics easy to learn?</td>\n",
       "      <td>How can you make physics easy to learn?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>27</td>\n",
       "      <td>28</td>\n",
       "      <td>What was your first sexual experience like?</td>\n",
       "      <td>What was your first sexual experience?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index  id  qid1  qid2  \\\n",
       "0  5      5   11    12     \n",
       "1  7      7   15    16     \n",
       "2  11     11  23    24     \n",
       "3  12     12  25    26     \n",
       "4  13     13  27    28     \n",
       "\n",
       "                                                                                question1  \\\n",
       "0  Astrology: I am a Capricorn Sun Cap moon and cap rising...what does that say about me?   \n",
       "1  How can I be a good geologist?                                                           \n",
       "2  How do I read and find my YouTube comments?                                              \n",
       "3  What can make Physics easy to learn?                                                     \n",
       "4  What was your first sexual experience like?                                              \n",
       "\n",
       "                                                                                    question2  \\\n",
       "0  I'm a triple Capricorn (Sun, Moon and ascendant in Capricorn) What does this say about me?   \n",
       "1  What should I do to be a great geologist?                                                    \n",
       "2  How can I see all my Youtube comments?                                                       \n",
       "3  How can you make physics easy to learn?                                                      \n",
       "4  What was your first sexual experience?                                                       \n",
       "\n",
       "   is_duplicate  \n",
       "0  1             \n",
       "1  1             \n",
       "2  1             \n",
       "3  1             \n",
       "4  1             "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duplicates = train_df[train_df[\"is_duplicate\"] == 1].reset_index()\n",
    "duplicate_pairs_num = len(duplicates)\n",
    "print(\"The number of duplicate question pairs: \" + str(duplicate_pairs_num))\n",
    "duplicates.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "duplicates_left = np.zeros((duplicate_pairs_num, max_question_length))\n",
    "duplicates_right = np.zeros((duplicate_pairs_num, max_question_length))\n",
    "\n",
    "for index, row in duplicates.iterrows():\n",
    "    tokens_left = process_question(row[\"question1\"])\n",
    "    indices_left = [word2index_map[token] for token in tokens_left]\n",
    "    duplicates_left[index, :] = pad_sequences([indices_left], maxlen=max_question_length)\n",
    "    \n",
    "    tokens_right = process_question(row[\"question2\"])\n",
    "    indices_right = [word2index_map[token] for token in tokens_right]\n",
    "    duplicates_right[index, :] = pad_sequences([indices_right], maxlen=max_question_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building an embedding layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The embedding layer is essentially one big matrix where each row is occupied with some word embedding (the correspondence between indices and words can be reestablished using dictionaries I define above) - in order to define the embedding layer keras blog turned out to be of great help (https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((len(index2word_map) + 1, WORD_EMBEDDING_DIM))\n",
    "for i in range(1, len(index2word_map) + 1):\n",
    "    if index2word_map[i] in w2v_embeddings:\n",
    "        embedding_matrix[i, :] = w2v_embeddings[index2word_map[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(input_dim=len(index2word_map)+1,\n",
    "                            output_dim=WORD_EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=max_question_length,\n",
    "                            trainable=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building encoder-decoder model in keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After I defined the embedding layer it is time to build my seq2seq model - this section is a heart of the notebook and it took me quite some time to get everything done\n",
    "\n",
    "The problem was: I already decided I want my model to predict the embeddings and not actual words - however, I could not really store them as then I would have tensor of (~400000, ~100, 100) and that is too much for my laptop - instead, I had to calculate these embeddings on the fly\n",
    "\n",
    "This calculation was rather tricky to implement - sparing the details and just giving out my solution to the problem: I have both original and duplicate questions as an input for my model - the original question goes through whole encoder-decoder pipeline and returns the suggested embeddings, the duplicate questions only goes through embedding layer - on top of them I add a dummy layer which explicitly calculates the loss I want (mean squared error) - on top of that I specify dummy loss function which simply takes the output of this final layer as an objective to optimize\n",
    "\n",
    "Because of that my model does not really have any target variables (my custom_loss makes no use of y_true), yet, in the fit stage I still had to provide some random array (with a proper shape) so that keras would not complain and just do the work\n",
    "\n",
    "Some links that helped me to go through this nightmare of a setup:\n",
    "* https://github.com/fchollet/keras/issues/4685\n",
    "* https://keras.io/getting-started/functional-api-guide/\n",
    "* https://github.com/fchollet/keras/issues/369\n",
    "* https://github.com/fchollet/keras/issues/1919\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "left_input = Input(shape=(max_question_length,))\n",
    "left_embedded_input = embedding_layer(left_input)\n",
    "encoded_question = LSTM(units=QUESTION_ENCODING_DIM, return_sequences=False)(left_embedded_input)\n",
    "padded_encoding = RepeatVector(max_question_length)(encoded_question)\n",
    "decoded_question = LSTM(units=WORD_EMBEDDING_DIM, return_sequences=True)(padded_encoding)\n",
    "\n",
    "right_input = Input(shape=(max_question_length,))\n",
    "right_embedded_input = embedding_layer(right_input)\n",
    "\n",
    "merged_embeddings = Concatenate(axis=2)([decoded_question, right_embedded_input])\n",
    "\n",
    "def calculate_mse_loss(x):\n",
    "    return keras.metrics.mean_squared_error(x[:, :, :WORD_EMBEDDING_DIM], x[:, :, WORD_EMBEDDING_DIM:])\n",
    "mse_loss_layer = Lambda(calculate_mse_loss, output_shape=(1,))(merged_embeddings)\n",
    "\n",
    "def mse_loss_placeholder(y_true, y_pred):\n",
    "    return y_pred\n",
    "\n",
    "lstm_model = Model(inputs=[left_input, right_input], outputs=mse_loss_layer)\n",
    "lstm_model.compile(loss=mse_loss_placeholder, optimizer=\"adam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if not os.path.isfile(\"base_encoder_weights.h5\"):\n",
    "    lstm_model.fit(x=[duplicates_left, duplicates_right], \n",
    "                   y=np.zeros((duplicates_left.shape[0], 1)), \n",
    "                   verbose=2,\n",
    "                   epochs=1\n",
    "                  )\n",
    "    lstm_model.save_weights(\"base_encoder_weights.h5\")        \n",
    "else:\n",
    "    lstm_model.load_weights(\"base_encoder_weights.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating and comparing question embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above defined and trained basic seq2seq model, below I want to compare how similar are derived encodings for duplicate and non-duplicate questions - to get the encoding I need to extract the output of intermediary LSTM layer in my model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<keras.engine.topology.InputLayer object at 0x00000053B2404EF0>, <keras.engine.topology.InputLayer object at 0x00000053D1127FD0>, <keras.layers.embeddings.Embedding object at 0x00000053B23F70B8>, <keras.layers.recurrent.LSTM object at 0x00000053CF2CFFD0>, <keras.layers.core.RepeatVector object at 0x00000053B1D7A2B0>, <keras.layers.recurrent.LSTM object at 0x00000053D0EB3828>, <keras.layers.merge.Concatenate object at 0x00000053D125D710>, <keras.layers.core.Lambda object at 0x00000053D125D3C8>]\n"
     ]
    }
   ],
   "source": [
    "print(lstm_model.layers)\n",
    "\n",
    "get_lstm_layer_output = K.function([lstm_model.layers[0].input], [lstm_model.layers[3].output])\n",
    "def get_question_embedding(x):\n",
    "    return get_lstm_layer_output([x])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def embed_original_string(original_string):\n",
    "    tokens = process_question(original_string)\n",
    "    indices = [word2index_map[token] for token in tokens]\n",
    "    indices = pad_sequences([indices], maxlen=max_question_length)\n",
    "    embedding = get_question_embedding(indices)\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if not os.path.isfile(\"similarities_base.pkl\"):\n",
    "    similarity_dict = {}\n",
    "    for index, row in train_df.iterrows():\n",
    "        embedding_left = embed_original_string(row[\"question1\"])\n",
    "        embedding_right = embed_original_string(row[\"question2\"])\n",
    "        embedding_similarity = cosine_similarity(embedding_left, embedding_right)\n",
    "        similarity_dict[index] = embedding_similarity[0][0]\n",
    "\n",
    "    with open(\"similarities_base.pkl\", 'wb') as f:\n",
    "        pickle.dump(similarity_dict, f, pickle.HIGHEST_PROTOCOL)        \n",
    "else:\n",
    "    with open(\"similarities_base.pkl\", \"rb\") as f:\n",
    "        similarity_dict = pickle.load(f)\n",
    "\n",
    "train_df[\"question_similarity\"] = pd.Series(similarity_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEICAYAAACwDehOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x53b23e80b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "duplicate_similarities = train_df[train_df[\"is_duplicate\"] == 1][\"question_similarity\"].values\n",
    "nonduplicate_similarities = train_df[train_df[\"is_duplicate\"] == 0][\"question_similarity\"].values\n",
    "\n",
    "plt.hist(duplicate_similarities, bins=100, normed=1, alpha=0.5, label=\"duplicates\")\n",
    "plt.hist(nonduplicate_similarities, bins=100, normed=1, alpha=0.5, label=\"non-duplicates\")\n",
    "plt.title(\"The distributions of cosine distances between embeddings\")\n",
    "plt.xlim([0.8, 1])\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The histograms above show that duplicate questions indeed tend to have more similar encodings (more mass is concentrated closer to identical embeddings where cosine distance is one), yet, it would be better if the distances for non-duplicate questions would be shifted more to the left\n",
    "\n",
    "Alarming fact: the spikes for identical embeddings - may be explained by the fact I calculate word embeddings from scratch and throw away tokens that do not occur frequently enough (without these rare tokens differnt questions may be compressed into the same token list)\n",
    "\n",
    "Also, I should have trained my RNN for more epochs: in previous reruns of this notebook I managed to get to better final state of the model - there the distribution of embedding distances for non-duplicate questions was centered at 0.95 and had more mass to the left (going back to 0.8 values) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examining pathological cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "      <th>question_similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>17</td>\n",
       "      <td>18</td>\n",
       "      <td>When do you use シ instead of し?</td>\n",
       "      <td>When do you use \"&amp;\" instead of \"and\"?</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>41</td>\n",
       "      <td>83</td>\n",
       "      <td>84</td>\n",
       "      <td>When can I expect my Cognizant confirmation mail?</td>\n",
       "      <td>When can I expect Cognizant confirmation mail?</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222</th>\n",
       "      <td>222</td>\n",
       "      <td>445</td>\n",
       "      <td>446</td>\n",
       "      <td>How can I find job in Japan?</td>\n",
       "      <td>How can I find an IT job in Japan?</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>277</th>\n",
       "      <td>277</td>\n",
       "      <td>554</td>\n",
       "      <td>555</td>\n",
       "      <td>How do most people die?</td>\n",
       "      <td>How do people die?</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>292</th>\n",
       "      <td>292</td>\n",
       "      <td>584</td>\n",
       "      <td>585</td>\n",
       "      <td>What is CPAGrip.com?</td>\n",
       "      <td>What is bestmytest.com?</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>298</td>\n",
       "      <td>596</td>\n",
       "      <td>597</td>\n",
       "      <td>On what online platforms can I post ads for beer money opportunity?</td>\n",
       "      <td>What online platforms can I post ads for beer money opportunity?</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>774</th>\n",
       "      <td>774</td>\n",
       "      <td>1543</td>\n",
       "      <td>1544</td>\n",
       "      <td>How is the word 'valiant' used in a sentence?</td>\n",
       "      <td>How is the word 'contorted' used in a sentence?</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>963</th>\n",
       "      <td>963</td>\n",
       "      <td>1921</td>\n",
       "      <td>1922</td>\n",
       "      <td>What kind of animal did this?</td>\n",
       "      <td>What kind of animal are you?</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1188</th>\n",
       "      <td>1188</td>\n",
       "      <td>2368</td>\n",
       "      <td>2369</td>\n",
       "      <td>How can I pass the HP0-M40 exam?</td>\n",
       "      <td>How can I pass the HP0-M33 exam?</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1195</th>\n",
       "      <td>1195</td>\n",
       "      <td>2382</td>\n",
       "      <td>2383</td>\n",
       "      <td>Who are you a role model for?</td>\n",
       "      <td>Who is your role model?</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1200</th>\n",
       "      <td>1200</td>\n",
       "      <td>2392</td>\n",
       "      <td>2393</td>\n",
       "      <td>Should I buy this new laptop or not?</td>\n",
       "      <td>Should I buy a new laptop?</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1202</th>\n",
       "      <td>1202</td>\n",
       "      <td>2396</td>\n",
       "      <td>2397</td>\n",
       "      <td>What is triloca.com?</td>\n",
       "      <td>What does Kakigarden.com do?</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1733</th>\n",
       "      <td>1733</td>\n",
       "      <td>3451</td>\n",
       "      <td>3452</td>\n",
       "      <td>Harvard College Courses: What is general shopping advice for HBTM classes?</td>\n",
       "      <td>Harvard College Courses: What is general shopping advice for SCRB classes?</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1877</th>\n",
       "      <td>1877</td>\n",
       "      <td>3737</td>\n",
       "      <td>3738</td>\n",
       "      <td>What is Kaufmich.com?</td>\n",
       "      <td>What does superbcrew.com do?</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2093</th>\n",
       "      <td>2093</td>\n",
       "      <td>4165</td>\n",
       "      <td>4166</td>\n",
       "      <td>What is the meaning of Urdu word 'Nasaaz'?</td>\n",
       "      <td>What is the meaning of Urdu word 'Ehteraam'?</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2249</th>\n",
       "      <td>2249</td>\n",
       "      <td>4474</td>\n",
       "      <td>2168</td>\n",
       "      <td>How is the word \"jocund\" used in a sentence?</td>\n",
       "      <td>How is the word 'inarticulate' used in a sentence?</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2461</th>\n",
       "      <td>2461</td>\n",
       "      <td>4891</td>\n",
       "      <td>4892</td>\n",
       "      <td>Who owns FinViz.com?</td>\n",
       "      <td>Who owns BillGates.com?</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3148</th>\n",
       "      <td>3148</td>\n",
       "      <td>6241</td>\n",
       "      <td>6242</td>\n",
       "      <td>How do I overcome my inferiority complex ?</td>\n",
       "      <td>How do I overcome my inferiority complex?</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3157</th>\n",
       "      <td>3157</td>\n",
       "      <td>6259</td>\n",
       "      <td>6260</td>\n",
       "      <td>Is sexstorian.com safe?</td>\n",
       "      <td>Is zerocensorship.com safe?</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3981</th>\n",
       "      <td>3981</td>\n",
       "      <td>7884</td>\n",
       "      <td>7885</td>\n",
       "      <td>How is the word 'furtive' used in a sentence?</td>\n",
       "      <td>How is the word 'prestidigitation' used in a sentence?</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  qid1  qid2  \\\n",
       "8     8     17    18     \n",
       "41    41    83    84     \n",
       "222   222   445   446    \n",
       "277   277   554   555    \n",
       "292   292   584   585    \n",
       "298   298   596   597    \n",
       "774   774   1543  1544   \n",
       "963   963   1921  1922   \n",
       "1188  1188  2368  2369   \n",
       "1195  1195  2382  2383   \n",
       "1200  1200  2392  2393   \n",
       "1202  1202  2396  2397   \n",
       "1733  1733  3451  3452   \n",
       "1877  1877  3737  3738   \n",
       "2093  2093  4165  4166   \n",
       "2249  2249  4474  2168   \n",
       "2461  2461  4891  4892   \n",
       "3148  3148  6241  6242   \n",
       "3157  3157  6259  6260   \n",
       "3981  3981  7884  7885   \n",
       "\n",
       "                                                                       question1  \\\n",
       "8     When do you use シ instead of し?                                              \n",
       "41    When can I expect my Cognizant confirmation mail?                            \n",
       "222   How can I find job in Japan?                                                 \n",
       "277   How do most people die?                                                      \n",
       "292   What is CPAGrip.com?                                                         \n",
       "298   On what online platforms can I post ads for beer money opportunity?          \n",
       "774   How is the word 'valiant' used in a sentence?                                \n",
       "963   What kind of animal did this?                                                \n",
       "1188  How can I pass the HP0-M40 exam?                                             \n",
       "1195  Who are you a role model for?                                                \n",
       "1200  Should I buy this new laptop or not?                                         \n",
       "1202  What is triloca.com?                                                         \n",
       "1733  Harvard College Courses: What is general shopping advice for HBTM classes?   \n",
       "1877  What is Kaufmich.com?                                                        \n",
       "2093  What is the meaning of Urdu word 'Nasaaz'?                                   \n",
       "2249  How is the word \"jocund\" used in a sentence?                                 \n",
       "2461  Who owns FinViz.com?                                                         \n",
       "3148  How do I overcome my inferiority complex ?                                   \n",
       "3157  Is sexstorian.com safe?                                                      \n",
       "3981  How is the word 'furtive' used in a sentence?                                \n",
       "\n",
       "                                                                       question2  \\\n",
       "8     When do you use \"&\" instead of \"and\"?                                        \n",
       "41    When can I expect Cognizant confirmation mail?                               \n",
       "222   How can I find an IT job in Japan?                                           \n",
       "277   How do people die?                                                           \n",
       "292   What is bestmytest.com?                                                      \n",
       "298   What online platforms can I post ads for beer money opportunity?             \n",
       "774   How is the word 'contorted' used in a sentence?                              \n",
       "963   What kind of animal are you?                                                 \n",
       "1188  How can I pass the HP0-M33 exam?                                             \n",
       "1195  Who is your role model?                                                      \n",
       "1200  Should I buy a new laptop?                                                   \n",
       "1202  What does Kakigarden.com do?                                                 \n",
       "1733  Harvard College Courses: What is general shopping advice for SCRB classes?   \n",
       "1877  What does superbcrew.com do?                                                 \n",
       "2093  What is the meaning of Urdu word 'Ehteraam'?                                 \n",
       "2249  How is the word 'inarticulate' used in a sentence?                           \n",
       "2461  Who owns BillGates.com?                                                      \n",
       "3148  How do I overcome my inferiority complex?                                    \n",
       "3157  Is zerocensorship.com safe?                                                  \n",
       "3981  How is the word 'prestidigitation' used in a sentence?                       \n",
       "\n",
       "      is_duplicate  question_similarity  \n",
       "8     0             1.0                  \n",
       "41    0             1.0                  \n",
       "222   0             1.0                  \n",
       "277   0             1.0                  \n",
       "292   0             1.0                  \n",
       "298   0             1.0                  \n",
       "774   0             1.0                  \n",
       "963   0             1.0                  \n",
       "1188  0             1.0                  \n",
       "1195  0             1.0                  \n",
       "1200  0             1.0                  \n",
       "1202  0             1.0                  \n",
       "1733  0             1.0                  \n",
       "1877  0             1.0                  \n",
       "2093  0             1.0                  \n",
       "2249  0             1.0                  \n",
       "2461  0             1.0                  \n",
       "3148  0             1.0                  \n",
       "3157  0             1.0                  \n",
       "3981  0             1.0                  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pathological_cases = train_df[train_df[\"is_duplicate\"] == 0]\n",
    "pathological_cases = pathological_cases[pathological_cases[\"question_similarity\"] == 1]\n",
    "pathological_cases.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explaining cases where non-duplicate questions have identical encodings (so that their cosine distance equals one) and some ideas how this situation can be tackled in follow-up analysis:\n",
    "* in general adding more data may lead to richer and more distinct embeddings: as of now I only look at duplicate pairs - however, in some sense every question is a duplicate of itself so I can greatly enhance my dataset my adding all unique questions from the test and training sets\n",
    "* one way to get better question encodings is to have backward decoding in the model (I assume it is a well-known trick but I have not implemented it so far)\n",
    "* in some cases the relevant bits could have been filtered with a stoplist - it might make sense to discard all stoplists whatsoever\n",
    "* in some cases (like for the first pair) punctuation symbols matter - even if I filter stuff out it might make sense to track what share of original question remains for encoding part \n",
    "* in some cases the difference lies in just two rare tokens: chances are there are no distinct embeddings for them which ensures identical question embeddings - might make sense to use pretrained word2vec vectors as starting weights before calibrating them to my corpus\n",
    "* in some cases the tokes are so rare I would not expect to see them among pretrained models: this is a real problem as even if I calculate embeddings for them, these embeddings would be the same (as tokens appear only once or so in identical contexts) so question embeddings would also be the same - to single out these cases I may need to refer to tf-idf features\n",
    "* in some cases non-duplicate questions are just the same and no model would differentiate them: because of that I will have to clip the probabilities I submit to Kaggle"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:dl_env]",
   "language": "python",
   "name": "conda-env-dl_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
